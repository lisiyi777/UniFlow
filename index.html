<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>UniFlow: Towards Zero-Shot LiDAR Scene Flow for Autonomous Vehicles via Cross-Domain Generalization</title>

  <style>
    :root {
      --text: #222;
      --muted: #555;
      --link: #0000EE;
      --bg: #ffffff;
      --max: 1000px;
      --rule: #e6e6e6;
      --card: #fafafa;
    }

    body {
      margin: 0;
      padding: 0;
      background: var(--bg);
      color: var(--text);
      font-family: "Times New Roman", Times, serif;
      font-size: 1.1rem;
      line-height: 1.6;
    }

    .container {
      max-width: var(--max);
      margin: 0 auto;
      padding: 28px 18px 70px;
    }

    h1,
    h2 {
      text-align: center;
      margin: 0;
      color: #111;
    }

    h1 {
      font-size: 44px;
      font-weight: 700;
      line-height: 1.2;
      margin-top: 18px;
      margin-bottom: 18px;
      letter-spacing: -0.02em;
    }

    h2 {
      font-size: 32px;
      font-weight: 700;
      margin: 44px 0 14px;
    }

    a {
      color: var(--link);
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
      text-underline-offset: 3px;
    }

    .center {
      text-align: center;
    }

    .authors {
      margin-top: 8px;
      font-size: 20px;
    }

    .authors a {
      color: #1a4fd8;
      font-weight: 600;
    }

    .affils {
      margin-top: 6px;
      color: #666;
      font-size: 16px;
    }

    .links {
      margin-top: 16px;
    }

    .links .pill {
      display: inline-block;
      padding: 8px 18px;
      margin: 0 6px;
      border-radius: 999px;
      background: #222;
      color: #fff;
      font-size: 16px;
      font-weight: 600;
      text-decoration: none;
    }

    .links .pill:hover {
      background: #000;
    }

    .hero {
      margin-bottom: 28px;
    }

    hr {
      border: 0;
      height: 1px;
      background: var(--rule);
      margin: 40px auto;
      width: 55%;
    }

    figure {
      margin: 18px auto 0;
      max-width: 980px;
    }

    figure img {
      width: 100%;
      height: auto;
      display: block;
      border-radius: 8px;
      box-shadow: 0 4px 14px rgba(0, 0, 0, 0.08);
      background: #fff;
    }

    figcaption {
      margin-top: 10px;
      color: var(--muted);
      font-size: 1.1rem;
      text-align: center;
    }

    .text {
      max-width: 900px;
      margin: 0 auto;
      text-align: justify;
    }

    .two {
      max-width: 980px;
      margin: 16px auto 0;
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 16px;
      align-items: start;
    }

    .panel {
      background: var(--card);
      border: 1px solid #eee;
      border-radius: 10px;
      padding: 14px;
    }

    .panel-title {
      font-weight: 700;
      margin: 0 0 6px;
      text-align: center;
    }

    video {
      width: 100%;
      display: block;
      border-radius: 8px;
      box-shadow: 0 4px 14px rgba(0, 0, 0, 0.08);
      background: #000;
    }

    .citation {
      background: #f5f5f5;
      border: 1px solid #e2e2e2;
      border-radius: 8px;
      padding: 14px 14px;
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      font-size: 14px;
      overflow-x: auto;
      max-width: 900px;
      margin: 0 auto;
      white-space: pre;
    }

    .footer {
      margin-top: 46px;
      color: #777;
      font-size: 14px;
      text-align: center;
    }

    .note {
      max-width: 900px;
      margin: 10px auto 0;
      color: var(--muted);
      font-size: 1.1rem;
      text-align: center;
    }

    .cvpr-table-wrap {
      overflow-x: auto;
      max-width: 980px;
      margin: 14px auto 0;
    }

    table.cvpr-table {
      border-collapse: collapse;
      width: 100%;
      font-size: 16px;
      background: #fff;
      border: 1px solid #eee;
      border-radius: 10px;
      overflow: hidden;
    }

    table.cvpr-table th,
    table.cvpr-table td {
      padding: 10px 10px;
      border-bottom: 1px solid #eee;
      text-align: center;
      white-space: nowrap;
    }

    table.cvpr-table thead th {
      background: #fafafa;
      font-weight: 700;
    }

    table.cvpr-table tbody tr:last-child td {
      border-bottom: none;
    }

    .cvpr-group td {
      text-align: center;
      font-weight: 700;
      background: #fcfcfc;
    }

    .cvpr-caption {
      max-width: 980px;
      margin: 10px auto 0;
      color: var(--muted);
      font-size: 16px;
      text-align: center;
    }

    /* --- video modal --- */
    .modal {
      position: fixed;
      inset: 0;
      background: rgba(0,0,0,0.72);
      display: none;
      align-items: center;
      justify-content: center;
      padding: 24px;
      z-index: 9999;
    }

    .modal.open { display: flex; }

    .modal-inner {
      width: min(1200px, 96vw);
      max-height: 90vh;
    }

    .modal video {
      width: 100%;
      height: auto;
      display: block;
      border-radius: 12px;
      background: #000;
      box-shadow: 0 10px 40px rgba(0,0,0,0.35);
    }

    .modal-close {
      position: fixed;
      top: 18px;
      right: 18px;
      width: 44px;
      height: 44px;
      border-radius: 999px;
      border: 0;
      background: rgba(255,255,255,0.12);
      color: #fff;
      font-size: 26px;
      cursor: pointer;
    }

    .modal-close:hover { background: rgba(255,255,255,0.18); }

    /* Make the inline video feel clickable */
    .clickable-media {
      cursor: pointer;
    }

    @media (max-width: 640px) {
     h2 { font-size: 26px; }
    }
    @media (max-width: 820px) {
      .two { grid-template-columns: 1fr; }
    }

    @media (max-width: 640px) {
      body { font-size: 17px; }
      h1 { font-size: 30px; }
      h2 { font-size: 22px; }
      .links { font-size: 18px; }
      figcaption { font-size: 15px; }
    }
  </style>
<header class="hero">
<body>
  <div class="container">
    <h1>UniFlow: Towards Zero-Shot LiDAR Scene Flow for Autonomous Vehicles via Cross-Domain Generalization</h1>

    <div class="center authors">
      Siyi Li<sup>1</sup>,
      <a href="https://kin-zhang.github.io/" target="_blank" rel="noreferrer">Qingwen Zhang</a><sup>2</sup>,
      <a href="https://ishan.khatri.io/" target="_blank" rel="noreferrer">Ishan Khatri</a><sup>3</sup>,
      <a href="https://vedder.io/" target="_blank" rel="noreferrer">Kyle Vedder</a><sup>1</sup>,
      <a href="https://www.cs.cmu.edu/~deva/" target="_blank" rel="noreferrer">Deva Ramanan</a><sup>3</sup>,
      <a href="https://www.neeharperi.com/" target="_blank" rel="noreferrer">Neehar Peri</a><sup>3</sup>
    </div>

    <div class="center affils">
      <sup>1</sup>University of Pennsylvania &nbsp;&nbsp;
      <sup>2</sup>KTH Royal Institute of Technology &nbsp;&nbsp;
      <sup>3</sup>Carnegie Mellon University
    </div>

    <div class="center links">
      <a class="pill" href="https://arxiv.org/abs/2511.18254" target="_blank">Paper</a>
      <a class="pill" href="https://github.com/lisiyi777/UniFlow" target="_blank">Code</a>
    </div>

    <figure>
      <img src="figures/teaser.png" alt="UniFlow teaser" />
      <figcaption>
         RGB views, LiDAR setups, and BEV point clouds from Argoverse 2, Waymo, nuScenes, and TruckScenes illustrate the cross-domain challenge of LiDAR scene flow.
      </figcaption>
    </figure>

    <hr />

    <h2>Abstract</h2>
    <div class="text">
      <p>
        LiDAR scene flow is the task of estimating per-point 3D motion between two point clouds. Recent methods achieve centimeter-level accuracy on popular autonomous vehicle (AV) datasets, but are typically only trained and evaluated on a single sensor. In this paper, we aim to learn general motion priors that transfer to diverse and unseen LiDAR sensors. However, prior work in LiDAR semantic segmentation and 3D object detection demonstrate that naively training on multiple datasets yields worse performance than single dataset models. 
      </p>
      </p>
        Interestingly, we find that this conventional wisdom does not hold for motion estimation, and that state-of-the-art scene flow methods greatly benefit from cross-dataset training. We posit that low-level tasks such as motion estimation may be less sensitive to sensor configuration; indeed, our analysis shows that models trained on fast-moving objects (e.g., from highway datasets) perform well on fast-moving objects, even across different datasets. 
      </p>
      </p>
        Informed by our analysis, we propose UniFlow, a family of feedforward models that unifies and trains on multiple large-scale LiDAR scene flow datasets with diverse sensor placements and point cloud densities. Our frustratingly simple solution establishes a new state-of-the-art on Waymo and nuScenes, improving over prior work by 5.1% and 35.2% respectively. Moreover, UniFlow achieves state-of-the-art accuracy on unseen datasets like TruckScenes, outperforming prior TruckScenes-specific models by 30.1%.        
      </p>
    </div>

    <hr />

    <h2>Cross-Dataset Generalization Correlates with Velocity Distribution</h2>

    <figure>
      <img src="figures/figure_two.png" alt="Velocity distribution and bucketed performance" />
      <figcaption>
        The velocity distributions for the AV2, Waymo, nuScenes, and TruckScenes train sets (top). The Dynamic Mean EPE per velocity bin of Flow4D trained on AV2, Waymo, nuScenes, TruckScenes, and UniFlow (bottom). Notably, Flow4D trained on TruckScenes outperforms Flow4D trained on any other dataset for fast-moving objects (2.0, ∞) across all datasets, as TruckScenes contains the largest number of fast-moving objects.
      </figcaption>
    </figure>

    <hr />

    <h2>Quantitative Results</h2>

    <div class="cvpr-table-wrap">
      <table class="cvpr-table" aria-label="Dynamic Bucket-Normalized Mean EPE across datasets">
        <thead>
          <tr>
            <th>Method</th>
            <th>AV2</th>
            <th>Waymo</th>
            <th>nuScenes</th>
            <th>TruckScenes</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>NSFP</td><td>0.422</td><td>0.574</td><td>0.602</td><td>0.658</td></tr>
          <tr><td>FastNSF</td><td>0.383</td><td>–</td><td>0.560</td><td>0.588</td></tr>
          <tr><td>SeFlow</td><td>0.309</td><td>0.328</td><td>0.554</td><td>0.681</td></tr>
          <tr><td>ICP Flow</td><td>0.331</td><td>–</td><td>–</td><td>0.472</td></tr>
          <!-- <tr><td>EulerFlow</td><td>0.130</td><td>–</td><td>–</td><td>–</td></tr> -->
          <!-- <tr><td>TrackFlow</td><td>0.269</td><td>–</td><td>–</td><td>–</td></tr> -->
          <tr><td>DeFlow</td><td>0.276</td><td>–</td><td>0.314</td><td>0.570</td></tr>
          <tr><td>SSF</td><td>0.181</td><td>0.264</td><td>0.220</td><td>0.453</td></tr>
          <tr><td>Flow4D</td><td>0.145</td><td>0.215</td><td>0.230</td><td>0.456</td></tr>
          <tr><td>&#916;Flow</td><td>0.113</td><td>0.198</td><td>0.216</td><td>0.402</td></tr>

          <tr class="cvpr-group">
            <td class="method-group">UniFlow (Ours)</td>
            <td class="group-spacer" colspan="5"></td>
          </tr>
          <tr><td>UniFlow-SSF</td><td>0.156</td><td>0.234</td><td>0.144</td><td>0.435</td></tr>
          <tr><td>UniFlow-Flow4D</td><td>0.132</td><td>0.191</td><td>0.196</td><td>0.281</td></tr>
          <tr><td>UniFlow-&#916;Flow</td><td>0.118</td><td>0.188</td><td>0.140</td><td>0.101</td></tr>
        </tbody>
      </table>
    </div>

    <div class="text">
      <p>
        <b>Comparison to State-of-the-Art Methods.</b>
        We compare UniFlow against recent scene flow methods using Dynamic Bucket-Normalized Mean EPE across multiple autonomous driving datasets.
        UniFlow consistently improves performance over single-dataset baselines, with especially strong gains on TruckScenes and nuScenes.
      </p>
    </div>

<hr />

<h2>Qualitative Results</h2>

<figure class="qual-figure">
  <img
    src="figures/qualitative_truckscenes.png"
    alt="Zero-shot generalization on TruckScenes: ΔFlow vs ΔFlow (UniFlow)"
  />
  <figcaption>
    Compared to dataset-specific &#916;Flow, &#916;Flow (UniFlow) produces more accurate motion estimates for rare objects
    and long-range vehicles under challenging conditions.
  </figcaption>
</figure>

<figure>
  <video id="qual-video" class="clickable-media" controls playsinline preload="metadata">
    <source src="figures/UniFlow_on_TruckScenes.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
</figure>

<div class="text">
  <p>
    Scene flow predictions on a challenging rainy sequence from TruckScenes. As shown above and in the video, &#916;Flow frequently produces artifacts on rain streaks and background points, which become especially pronounced during occlusions, and predicts inconsistent flow vectors on dynamic objects. In contrast, &#916;Flow (UniFlow) yields significantly more stable and coherent motion fields.
  </p>
</div>

<hr>

<section id="cvpr-challenge">
  <h2>CVPR 2026 Challenge</h2>
  <div class="text">
    <p>
      We are hosting a challenge at CVPR 2026 to encourage broad community involvement in addressing long-range (up to 75m)
      LiDAR scene flow across diverse AV datasets. Participants are allowed to train their models on any publicly available
      datasets (including TruckScenes) and will be evaluated on <b>Argoverse 2</b>, <b>nuScenes</b>, <b>Waymo</b>, and <b>TruckScenes</b>.
      We include supervised and unsupervised baselines above (Dynamic Bucket-Normalized; lower is better).
    </p>
  </div>
  <div class="cvpr-table-wrap">
    <table class="cvpr-table" aria-label="CVPR 2026 Challenge Baselines">
      <thead>
        <tr>
          <th>Methods</th>
          <th>Mean</th>
          <th>AV2</th>
          <th>Waymo</th>
          <th>nuScenes</th>
          <th>TruckScenes</th>
        </tr>
      </thead>
      <tbody>
        <tr class="cvpr-group">
          <td class="method-group">Unsupervised</td>
          <td class="group-spacer" colspan="5"></td>
        </tr>
        <tr><td>NSFP</td><td>0.5642</td><td>0.4219</td><td>0.5740</td><td>0.6024</td><td>0.6583</td></tr>
        <tr><td>FastNSF</td><td>0.4971</td><td>0.3826</td><td>0.4576</td><td>0.5597</td><td>0.5884</td></tr>
        <tr><td>SeFlow</td><td>0.4710</td><td>0.3085</td><td>0.3509</td><td>0.5440</td><td>0.6808</td></tr>
        <tr><td>TeFlow</td><td>0.3465</td><td>0.2051</td><td>0.2747</td><td>0.3954</td><td>0.5108</td></tr>
        <tr class="cvpr-group">
          <td class="method-group">Supervised</td>
          <td class="group-spacer" colspan="5"></td>
        </tr>
        <tr><td>SSF</td><td>0.2764</td><td>0.2651</td><td>0.2451</td><td>0.1955</td><td>0.4000</td></tr>
        <tr><td>Flow4D</td><td>0.2511</td><td>0.2136</td><td>0.2047</td><td>0.2384</td><td>0.3476</td></tr>
        <tr><td>&#916;Flow</td><td>0.2391</td><td>0.1901</td><td>0.1959</td><td>0.2196</td><td>0.3500</td></tr>

        <tr><td>SSF (UniFlow, Ours)</td><td>0.2469</td><td>0.2193</td><td>0.2274</td><td>0.2498</td><td>0.2911</td></tr>
        <tr><td>Flow4D-XL (UniFlow, Ours)</td><td>0.2416</td><td>0.2050</td><td>0.1890</td><td>0.2852</td><td>0.2872</td></tr>
        <tr><td>&#916;Flow (UniFlow, Ours)</td><td>0.2222</td><td>0.1831</td><td>0.1879</td><td>0.2526</td><td>0.2651</td></tr>
      </tbody>
    </table>
  </div>

</section>

    <h2>Citation</h2>
    <div class="citation">@misc{li2025uniflowzeroshotlidarscene,
      title={UniFlow: Towards Zero-Shot LiDAR Scene Flow for Autonomous Vehicles via Cross-Domain Generalization},
      author={Siyi Li and Qingwen Zhang and Ishan Khatri and Kyle Vedder and Deva Ramanan and Neehar Peri},
      year={2025},
      eprint={2511.18254},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2511.18254}
}
    </div>
    <div class="footer">
      © <span id="y"></span> UniFlow Authors · Last updated <span id="d"></span>
    </div>
  </div>

  <div id="video-modal" class="modal" aria-hidden="true">
    <button class="modal-close" id="video-modal-close" aria-label="Close">×</button>
    <div class="modal-inner">
      <video id="video-modal-player" controls preload="metadata">
        <!-- source will be set by JS -->
      </video>
    </div>
  </div>

  <script>
    // --- footer date ---
    (function () {
      const now = new Date();
      const y = document.getElementById('y');
      const d = document.getElementById('d');
      if (y) y.textContent = String(now.getFullYear());
      if (d) {
        d.textContent = new Intl.DateTimeFormat(undefined, {
          year: 'numeric', month: 'short', day: '2-digit'
        }).format(now);
      }
    })();

    // --- video modal logic ---
    (function () {
      const inlineVideo = document.getElementById('qual-video');
      const modal = document.getElementById('video-modal');
      const modalPlayer = document.getElementById('video-modal-player');
      const closeBtn = document.getElementById('video-modal-close');

      if (!inlineVideo || !modal || !modalPlayer || !closeBtn) return;

      function openVideoModal(src) {
        modal.classList.add('open');
        modal.setAttribute('aria-hidden', 'false');
        modalPlayer.innerHTML = `<source src="${src}" type="video/mp4">`;
        modalPlayer.load();
        modalPlayer.play().catch(() => {});
      }

      function closeVideoModal() {
        modal.classList.remove('open');
        modal.setAttribute('aria-hidden', 'true');
        modalPlayer.pause();
        modalPlayer.innerHTML = '';
      }

      // Clicking the inline video opens modal
      inlineVideo.addEventListener('click', () => {
        const source = inlineVideo.querySelector('source');
        if (source && source.getAttribute('src')) {
          // Use the attribute value so it works even if browser doesn't expand source.src
          openVideoModal(source.getAttribute('src'));
        }
      });

      closeBtn.addEventListener('click', closeVideoModal);

      // Click outside video closes
      modal.addEventListener('click', (e) => {
        if (e.target === modal) closeVideoModal();
      });

      // ESC closes
      document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape' && modal.classList.contains('open')) closeVideoModal();
      });
    })();
  </script>


</body>

</html>
