<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>UniFlow: Towards Zero-Shot LiDAR Scene Flow for Autonomous Vehicles via Cross-Domain Generalization</title>

  <style>
    :root {
      --text: #222;
      --muted: #555;
      --link: #0000EE;
      --bg: #ffffff;
      --max: 1000px;
      --rule: #e6e6e6;
      --card: #fafafa;
    }

    body {
      margin: 0;
      padding: 0;
      background: var(--bg);
      color: var(--text);
      font-family: "Times New Roman", Times, serif;
      font-size: 18px;
      line-height: 1.6;
    }

    .container {
      max-width: var(--max);
      margin: 0 auto;
      padding: 28px 18px 70px;
    }

    h1,
    h2 {
      text-align: center;
      margin: 0;
      color: #111;
    }

    h1 {
      font-size: 40px;
      font-weight: 700;
      line-height: 1.15;
      margin-top: 10px;
    }

    h2 {
      font-size: 26px;
      font-weight: 700;
      margin: 44px 0 14px;
    }

    a {
      color: var(--link);
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
      text-underline-offset: 3px;
    }

    .center {
      text-align: center;
    }

    .authors {
      margin-top: 10px;
      font-size: 18px;
    }

    .authors a {
      font-weight: 600;
    }

    .affils {
      margin-top: 6px;
      color: var(--muted);
      font-size: 17px;
    }

    .links {
      margin-top: 12px;
      font-size: 20px;
    }

    .links a {
      font-weight: 600;
    }

    .links span {
      display: inline-block;
      margin: 0 8px;
    }

    hr {
      border: 0;
      height: 1px;
      background: var(--rule);
      margin: 40px auto;
      width: 55%;
    }

    figure {
      margin: 18px auto 0;
      max-width: 980px;
    }

    figure img {
      width: 100%;
      height: auto;
      display: block;
      border-radius: 8px;
      box-shadow: 0 4px 14px rgba(0, 0, 0, 0.08);
      background: #fff;
    }

    figcaption {
      margin-top: 10px;
      color: var(--muted);
      font-size: 16px;
      text-align: center;
    }

    .text {
      max-width: 900px;
      margin: 0 auto;
      text-align: justify;
    }

    .two {
      max-width: 980px;
      margin: 16px auto 0;
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 16px;
      align-items: start;
    }

    .panel {
      background: var(--card);
      border: 1px solid #eee;
      border-radius: 10px;
      padding: 14px;
    }

    .panel-title {
      font-weight: 700;
      margin: 0 0 6px;
      text-align: center;
    }

    video {
      width: 100%;
      display: block;
      border-radius: 8px;
      box-shadow: 0 4px 14px rgba(0, 0, 0, 0.08);
      background: #000;
    }

    .citation {
      background: #f5f5f5;
      border: 1px solid #e2e2e2;
      border-radius: 8px;
      padding: 14px 14px;
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      font-size: 14px;
      overflow-x: auto;
      max-width: 900px;
      margin: 0 auto;
      white-space: pre;
    }

    .footer {
      margin-top: 46px;
      color: #777;
      font-size: 14px;
      text-align: center;
    }

    @media (max-width: 820px) {
      .two { grid-template-columns: 1fr; }
    }

    @media (max-width: 640px) {
      body { font-size: 17px; }
      h1 { font-size: 30px; }
      h2 { font-size: 22px; }
      .links { font-size: 18px; }
      figcaption { font-size: 15px; }
    }
  </style>
</head>

<body>
  <div class="container">
    <h1>UniFlow: Towards Zero-Shot LiDAR Scene Flow for Autonomous Vehicles via Cross-Domain Generalization</h1>

    <div class="center authors">
      Siyi Li<sup>1</sup>,
      <a href="https://kin-zhang.github.io/" target="_blank" rel="noreferrer">Qingwen Zhang</a><sup>2</sup>,
      <a href="https://ishan.khatri.io/" target="_blank" rel="noreferrer">Ishan Khatri</a><sup>3</sup>,
      <a href="https://vedder.io/" target="_blank" rel="noreferrer">Kyle Vedder</a><sup>1</sup>,
      <a href="https://www.cs.cmu.edu/~deva/" target="_blank" rel="noreferrer">Deva Ramanan</a><sup>3</sup>,
      <a href="https://www.neeharperi.com/" target="_blank" rel="noreferrer">Neehar Peri</a><sup>3</sup>
    </div>

    <div class="center affils">
      <sup>1</sup>University of Pennsylvania &nbsp;&nbsp;
      <sup>2</sup>KTH Royal Institute of Technology &nbsp;&nbsp;
      <sup>3</sup>Carnegie Mellon University
    </div>

    <div class="center links" aria-label="project links">
      <span>[<a href="https://arxiv.org/abs/2511.18254" target="_blank" rel="noreferrer">Paper</a>]</span>
      <span>[<a href="https://github.com/lisiyi777/UniFlow" target="_blank" rel="noreferrer">Code</a>]</span>
    </div>

    <!-- HERO: Use Figure 1, a video hero, or a custom teaser you make -->
    <figure>
      <img src="figures/teaser.png" alt="UniFlow teaser" />
      <figcaption>
        <b>Teaser.</b> Unified multi-dataset training improves LiDAR scene flow generalization across sensors and environments.
        (Replace <code>figures/teaser.png</code> with Figure 1 / a custom teaser.)
      </figcaption>
    </figure>

    <hr />

    <h2>Abstract</h2>
    <div class="text">
      <p>
        LiDAR scene flow is the task of estimating per-point 3D motion between two point clouds. Recent methods achieve centimeter-level accuracy on popular autonomous vehicle (AV) datasets, but are typically only trained and evaluated on a single sensor. In this paper, we aim to learn general motion priors that transfer to diverse and unseen LiDAR sensors. However, prior work in LiDAR semantic segmentation and 3D object detection demonstrate that naively training on multiple datasets yields worse performance than single dataset models. Interestingly, we find that this conventional wisdom does not hold for motion estimation, and that state-of-the-art scene flow methods greatly benefit from cross-dataset training. We posit that low-level tasks such as motion estimation may be less sensitive to sensor configuration; indeed, our analysis shows that models trained on fast-moving objects (e.g., from highway datasets) perform well on fast-moving objects, even across different datasets. Informed by our analysis, we propose UniFlow, a family of feedforward models that unifies and trains on multiple large-scale LiDAR scene flow datasets with diverse sensor placements and point cloud densities. Our frustratingly simple solution establishes a new state-of-the-art on Waymo and nuScenes, improving over prior work by 5.1% and 35.2% respectively. Moreover, UniFlow achieves state-of-the-art accuracy on unseen datasets like TruckScenes, outperforming prior TruckScenes-specific models by 30.1%.        
      </p>
    </div>

    <h2>A. Why UniFlow?</h2>
    <div class="text">
      <p>
        UniFlow is an insight-driven study on cross-domain generalization for LiDAR scene flow.
        Unlike LiDAR semantics and detection, scene flow estimates low-level geometric motion and is less sensitive to dataset taxonomies.
        This suggests that motion priors learned from diverse datasets can transfer across sensors and environments.
      </p>
    </div>

    <!-- SECTION A VISUAL: a simple conceptual diagram contrasting semantics vs motion -->
    <figure>
      <img src="figures/sectionA_concept.png" alt="Conceptual comparison: semantics vs motion" />
      <figcaption>
        <b>Concept.</b> Scene flow is a low-level geometric task; motion priors are more transferable than dataset-specific semantic taxonomies.
      </figcaption>
    </figure>

    <h2>B. Unified training works (across models)</h2>
    <div class="text">
      <p>
        UniFlow retrains multiple strong supervised scene flow backbones under a unified multi-dataset regime (AV2, Waymo, nuScenes),
        without changing architectures.
        This yields consistent in-domain gains and strong zero-shot performance on TruckScenes.
      </p>
    </div>

    <!-- SECTION B VISUAL: one compact headline table/plot from the paper -->
    <figure>
      <img src="figures/sectionB_headline_results.png" alt="Headline results table/plot" />
      <figcaption>
        <b>Headline results.</b> In-domain improvements (Waymo, nuScenes) and zero-shot generalization to TruckScenes.
      </figcaption>
    </figure>

    <h2>C. What drives generalization?</h2>
    <div class="text">
      <p>
        A central finding of UniFlow is that cross-domain performance is strongly tied to the velocity distribution observed during training.
        Figure 2 connects dataset motion statistics to accuracy across velocity buckets.
      </p>
    </div>

    <!-- SECTION C: use Figure 2 -->
    <figure>
      <img src="figures/figure_two.png" alt="Figure 2: velocity distributions and generalization" />
      <figcaption>
        <b>Figure 2.</b> Generalization correlates with motion statistics (velocity distribution / bucketed performance).
      </figcaption>
    </figure>

    <h2>Video</h2>
    <div class="text">
      <p>
        We include an illustrative video demonstrating UniFlow’s qualitative behavior and cross-domain robustness.
      </p>
    </div>

    <figure>
      <video controls playsinline preload="metadata">
        <source src="figures/UniFlow_on_TruckScenes.mp4" type="video/mp4" />
        Your browser does not support the video tag.
      </video>
      <figcaption>
        <b>Qualitative video.</b>
      </figcaption>
    </figure>

    <h2>Citation</h2>
    <div class="citation">@misc{li2025uniflowzeroshotlidarscene,
      title={UniFlow: Towards Zero-Shot LiDAR Scene Flow for Autonomous Vehicles via Cross-Domain Generalization},
      author={Siyi Li and Qingwen Zhang and Ishan Khatri and Kyle Vedder and Deva Ramanan and Neehar Peri},
      year={2025},
      eprint={2511.18254},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2511.18254}
}
    </div>
    <div class="footer">
      © <span id="y"></span> UniFlow Authors · Last updated <span id="d"></span>
    </div>
  </div>

  <script>
    const now = new Date();
    document.getElementById('y').textContent = String(now.getFullYear());
    document.getElementById('d').textContent = new Intl.DateTimeFormat(undefined, {
      year: 'numeric', month: 'short', day: '2-digit'
    }).format(now);
  </script>
</body>

</html>
